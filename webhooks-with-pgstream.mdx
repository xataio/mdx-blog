---
title: 'Postgres webhooks with pgstream'
description: 'A simple tutorial for calling webhooks on Postgres data and schema events using pgstream.'
image:
  src: https://raw.githubusercontent.com/xataio/mdx-blog/main/images/pgstream/introducing-pgstream@2x.jpg
  alt: pgstream
author: Tudor Golubenco
authorEmail: tudor@xata.io
date: 08-12-2024
tags: ['postgres', 'fpPgstream', 'oss', 'open-source']
published: true
slug: postgres-webhooks-with-pgstream
ogImage: https://raw.githubusercontent.com/xataio/mdx-blog/main/images/pgstream/introducing-pgstream@2x.jpg
---

[pgstream]() is a CDC (Change-Data-Capture) tool focused on PostgreSQL. It's main differentiator, as of now, is that it is capable of capturing also schema changes, in addition to data changes. It also has an advanced Elasticsearch/OpenSearch output, which takes care of a number of common gotchas for the common use case of Postgres -> Elasticsearch replication.

In this post, we're going to show how you can use pgstream to call simple webhooks whenever there are data (or schema) changes in Postgres.

## Preparation

For this tutorial we need a Postgres instance that has logical replication enabled and the [wal2json](https://github.com/eulerto/wal2json) output plugin loaded. There are a number of ways to do this, but we're going to use the [docker-compose file](https://github.com/xataio/pgstream/blob/main/build/docker/docker-compose.yml) from the pgstream repo:

```sh
git clone https://github.com/xataio/pgstream.git
```

And then start the Postgres instance:

```sh
docker compose -f build/docker/docker-compose.yml up db
```

Then you can connect to it and create a simple table to play with:

```sh
psql "postgres://postgres:postgres@localhost:5432/postgres"
```

At the psql prompt:

```sql
CREATE TABLE users(id SERIAL PRIMARY KEY, name TEXT);
```

Let's now install `pgstream`. One option is to build it from source, since we already have the repo cloned:

```sh
go build
```

The above assumes you have Go installed. If you don't, it might be easier to use the Homebrew tap:

```sh
brew tap xataio/pgstream
brew install pgstream
```

Or you can find pre-built binaries on the [release pages](https://github.com/xataio/pgstream/releases).

Next step is to initialize `pgstream`, like this:

```sh
pgstream init --pgurl "postgres://postgres:postgres@localhost/postgres?sslmode=disable"
```

## Configuration

The `pgstream` repo comes with a `pg2webhook.env` configuration file for this purpose. Its contents are:

```sh
# Listener config
PGSTREAM_POSTGRES_LISTENER_URL="postgres://postgres:postgres@localhost?sslmode=disable"

# Processor config
PGSTREAM_WEBHOOK_SUBSCRIPTION_STORE_URL="postgres://postgres:postgres@localhost?sslmode=disable"
PGSTREAM_WEBHOOK_SUBSCRIPTION_STORE_CACHE_ENABLED=false
PGSTREAM_WEBHOOK_SUBSCRIPTION_STORE_CACHE_REFRESH_INTERVAL="60s"
```

In this file, there are two Postgres connection strings defined. The first one is for the listener, to which database should pgstream connect and listen for changes. The second one is used as a metadata db to store the registered subscriptions and their configuration. In this example, they are both set to the same database, but you can use different databases or instances if you prefer.

You are now ready to start `pgstream` like this:

```sh
pgstream run -c pg2webhook.env
```

## The webhooks listener

We now have pgstream ready to go, but we need something to receive the webhooks. Let's use this simple python program (courtesy of ChatGPT) to start a webserver that logs all the requests it receives:

```python
import http.server
import json
from urllib.parse import urlparse, parse_qs

class WebhookHandler(http.server.BaseHTTPRequestHandler):
    def do_POST(self):
        # Parse the length of the data
        content_length = int(self.headers['Content-Length'])

        # Read the data from the request
        post_data = self.rfile.read(content_length)

        # Try to parse the data as JSON
        try:
            json_data = json.loads(post_data)
            pretty_json = json.dumps(json_data, indent=4)
            print("Received Webhook:")
            print(pretty_json)
        except json.JSONDecodeError:
            print("Received non-JSON data:")
            print(post_data.decode('utf-8'))

        # Send a response back to the webhook sender
        self.send_response(200)
        self.send_header('Content-Type', 'text/plain')
        self.end_headers()
        self.wfile.write(b'Webhook received')

def run(server_class=http.server.HTTPServer, handler_class=WebhookHandler, port=9910):
    server_address = ('', port)
    httpd = server_class(server_address, handler_class)
    print(f'Starting server on port {port}...')
    httpd.serve_forever()

if __name__ == '__main__':
    run()
```

You can start it like this:

```sh
python3 webhooks_server.py

Starting server on port 9910...
```

Now we have to create a subscription so that `pgstream` knows to send the webhook to `localhost:9910` where the `webhook_server.go` program is waiting. We do this by calling an API which the webhook module from `pgstream` serves:

```sh
curl localhost:9900/webhooks/subscribe -H "content-type: application/json" -d '{"url":"http://localhost:9910", "schema": "public", "table": "users"}'
```

The above adds a line in the `webhook_subscriptions` table, which is maintained by `pgstream`. You can verify by running via `psql`:

```sql
SELECT * FROM webhook_subscriptions;
url          | schema_name | table_name | event_types
-----------------------+-------------+------------+-------------
 http://localhost:9910 | public      | users      |
(1 row)
```

## Simple data updates

With `pgstream` and the `webhooks_server.py` started in different consoles, let's run over `psql`:

```sql
INSERT INTO users(name) VALUES('Michael');
```

You should see the following printed by the python program:

```json
{
  "Data": {
    "action": "I",
    "timestamp": "2024-08-12 13:54:43.555761+00",
    "lsn": "0/15FA710",
    "schema": "public",
    "table": "users",
    "columns": [
      {
        "id": "",
        "name": "id",
        "type": "integer",
        "value": 6
      },
      {
        "id": "",
        "name": "name",
        "type": "text",
        "value": "Michael"
      }
    ],
    "identity": null,
    "metadata": {
      "schema_id": null,
      "table_pgstream_id": "",
      "id_col_pgstream_id": null,
      "version_col_pgstream_id": ""
    }
  }
}
```

Things to note in the above:

- The "action" is `I` for "insert".
- The LSN (Log Sequence Number) is is included. This is a unique and sorted value generated by Postgres, which you can use to order the events in case they are processed out of order.
- Each column is included with their name, type, new value, and an ID generated by pgstream. This ID is unique for each column and can be used to follow columns across renames.

### Update

## Including old values

## Schema change events

## Conclusion / next steps
